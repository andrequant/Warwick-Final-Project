{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "from jax import grad, vmap, random\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Parameters\n",
    "sigma_1 = 0.5\n",
    "v_max = 2.0\n",
    "rho_max = 1.0\n",
    "a = 0.9\n",
    "b = 0.975\n",
    "L = 10.0\n",
    "N = 100\n",
    "dx = L / (N-1)\n",
    "x = jnp.linspace(0, L, N)\n",
    "T = 1.0  # final time\n",
    "dt = 0.01\n",
    "time_steps = int(T/dt)\n",
    "\n",
    "# Initial condition\n",
    "rho_init = jnp.zeros(N)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "\n",
    "class RhoNN(eqx.Module):\n",
    "    layers: list\n",
    "    final_layer: eqx.nn.Linear\n",
    "\n",
    "    def __init__(self, key):\n",
    "        keys = jax.random.split(key, 4)\n",
    "        self.layers = [\n",
    "            eqx.nn.Linear(2, 64, key=keys[0]),\n",
    "            eqx.nn.Linear(64, 64, key=keys[1]),\n",
    "            eqx.nn.Linear(64, 64, key=keys[2]),\n",
    "        ]\n",
    "        self.final_layer = eqx.nn.Linear(64, 1, key=keys[3])\n",
    "\n",
    "    def __call__(self, x, t):\n",
    "        xt = jnp.concatenate([x, t], axis=-1)\n",
    "        for layer in self.layers:\n",
    "            xt = jax.nn.relu(layer(xt))\n",
    "        return self.final_layer(xt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pde_loss(rho_n, x, t, sigma_1, v_max, rho_max):\n",
    "    grad_rho_x = jax.grad(lambda x, t: rho_n(x, t), argnums=0)(x, t)\n",
    "    grad_rho_t = jax.grad(lambda x, t: rho_n(x, t), argnums=1)(x, t)\n",
    "    laplacian_rho = jax.grad(jax.grad(rho_n, argnums=0), argnums=0)(x, t)\n",
    "    \n",
    "    f_rho = v_max * (1 - 2 * rho_n(x, t) / rho_max)\n",
    "    pde_residual = grad_rho_t - sigma_1 * laplacian_rho + grad_rho_x * f_rho\n",
    "    return jnp.mean(pde_residual ** 2)\n",
    "\n",
    "def boundary_loss(rho_n, x_in, x_out, t, a, b, rho_max):\n",
    "    loss_in = jnp.mean((rho_n(x_in, t) - a * (rho_max - rho_n(x_in, t))) ** 2)\n",
    "    loss_out = jnp.mean((rho_n(x_out, t) - b * rho_n(x_out, t)) ** 2)\n",
    "    return loss_in + loss_out\n",
    "\n",
    "def total_loss(rho_n, x, t, x_in, x_out, sigma_1, v_max, rho_max, a, b):\n",
    "    return pde_loss(rho_n, x, t, sigma_1, v_max, rho_max) + boundary_loss(rho_n, x_in, x_out, t, a, b, rho_max)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Cannot concatenate arrays with shapes that differ in dimensions other than the one being concatenated: concatenating along dimension 1 for shapes (100, 1), (10000, 1).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 20\u001b[0m\n\u001b[0;32m     17\u001b[0m x_out \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39marray([[L]])\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1000\u001b[39m):\n\u001b[1;32m---> 20\u001b[0m     rho_0, opt_state, loss_value \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrho_0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     22\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss_value\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[38], line 9\u001b[0m, in \u001b[0;36mstep\u001b[1;34m(rho_n, opt_state, x, t, x_in, x_out, sigma_1, v_max, rho_max, a, b)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(rho_n, opt_state, x, t, x_in, x_out, sigma_1, v_max, rho_max, a, b):\n\u001b[1;32m----> 9\u001b[0m     loss, grads \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_loss\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrho_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m     updates, opt_state \u001b[38;5;241m=\u001b[39m optimizer\u001b[38;5;241m.\u001b[39mupdate(grads, opt_state, rho_n)\n\u001b[0;32m     11\u001b[0m     rho_n \u001b[38;5;241m=\u001b[39m eqx\u001b[38;5;241m.\u001b[39mapply_updates(rho_n, updates)\n",
      "    \u001b[1;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[37], line 16\u001b[0m, in \u001b[0;36mtotal_loss\u001b[1;34m(rho_n, x, t, x_in, x_out, sigma_1, v_max, rho_max, a, b)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtotal_loss\u001b[39m(rho_n, x, t, x_in, x_out, sigma_1, v_max, rho_max, a, b):\n\u001b[1;32m---> 16\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpde_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrho_n\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho_max\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m boundary_loss(rho_n, x_in, x_out, t, a, b, rho_max)\n",
      "Cell \u001b[1;32mIn[37], line 2\u001b[0m, in \u001b[0;36mpde_loss\u001b[1;34m(rho_n, x, t, sigma_1, v_max, rho_max)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpde_loss\u001b[39m(rho_n, x, t, sigma_1, v_max, rho_max):\n\u001b[1;32m----> 2\u001b[0m     grad_rho_x \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho_n\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     grad_rho_t \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mgrad(\u001b[38;5;28;01mlambda\u001b[39;00m x, t: rho_n(x, t), argnums\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)(x, t)\n\u001b[0;32m      4\u001b[0m     laplacian_rho \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mgrad(jax\u001b[38;5;241m.\u001b[39mgrad(rho_n, argnums\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), argnums\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)(x, t)\n",
      "    \u001b[1;31m[... skipping hidden 10 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[37], line 2\u001b[0m, in \u001b[0;36mpde_loss.<locals>.<lambda>\u001b[1;34m(x, t)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpde_loss\u001b[39m(rho_n, x, t, sigma_1, v_max, rho_max):\n\u001b[1;32m----> 2\u001b[0m     grad_rho_x \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mgrad(\u001b[38;5;28;01mlambda\u001b[39;00m x, t: \u001b[43mrho_n\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m, argnums\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)(x, t)\n\u001b[0;32m      3\u001b[0m     grad_rho_t \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mgrad(\u001b[38;5;28;01mlambda\u001b[39;00m x, t: rho_n(x, t), argnums\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)(x, t)\n\u001b[0;32m      4\u001b[0m     laplacian_rho \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mgrad(jax\u001b[38;5;241m.\u001b[39mgrad(rho_n, argnums\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), argnums\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)(x, t)\n",
      "Cell \u001b[1;32mIn[36], line 19\u001b[0m, in \u001b[0;36mRhoNN.__call__\u001b[1;34m(self, x, t)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, t):\n\u001b[1;32m---> 19\u001b[0m     xt \u001b[38;5;241m=\u001b[39m \u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m     21\u001b[0m         xt \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mrelu(layer(xt))\n",
      "File \u001b[1;32mc:\\Users\\andre\\OneDrive\\Documentos\\Warwick\\Individual Project\\Code\\Warwick-Final-Project\\venv\\Lib\\site-packages\\jax\\_src\\numpy\\lax_numpy.py:2911\u001b[0m, in \u001b[0;36mconcatenate\u001b[1;34m(arrays, axis, dtype)\u001b[0m\n\u001b[0;32m   2909\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m16\u001b[39m\n\u001b[0;32m   2910\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(arrays_out) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m-> 2911\u001b[0m   arrays_out \u001b[38;5;241m=\u001b[39m [\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2912\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(arrays_out), k)]\n\u001b[0;32m   2913\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arrays_out[\u001b[38;5;241m0\u001b[39m]\n",
      "    \u001b[1;31m[... skipping hidden 27 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\andre\\OneDrive\\Documentos\\Warwick\\Individual Project\\Code\\Warwick-Final-Project\\venv\\Lib\\site-packages\\jax\\_src\\lax\\lax.py:3362\u001b[0m, in \u001b[0;36m_concatenate_shape_rule\u001b[1;34m(*operands, **kwargs)\u001b[0m\n\u001b[0;32m   3358\u001b[0m   msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot concatenate arrays with shapes that differ in dimensions \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3359\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mother than the one being concatenated: concatenating along \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3360\u001b[0m          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdimension \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m for shapes \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   3361\u001b[0m   shapes \u001b[38;5;241m=\u001b[39m [operand\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mfor\u001b[39;00m operand \u001b[38;5;129;01min\u001b[39;00m operands]\n\u001b[1;32m-> 3362\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg\u001b[38;5;241m.\u001b[39mformat(dimension, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m, shapes))))\n\u001b[0;32m   3364\u001b[0m concat_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(o\u001b[38;5;241m.\u001b[39mshape[dimension] \u001b[38;5;28;01mfor\u001b[39;00m o \u001b[38;5;129;01min\u001b[39;00m operands)\n\u001b[0;32m   3365\u001b[0m ex_shape \u001b[38;5;241m=\u001b[39m operands[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mTypeError\u001b[0m: Cannot concatenate arrays with shapes that differ in dimensions other than the one being concatenated: concatenating along dimension 1 for shapes (100, 1), (10000, 1)."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import optax\n",
    "\n",
    "key = jax.random.PRNGKey(0)\n",
    "rho_0 = RhoNN(key)\n",
    "optimizer = optax.adam(1e-3)\n",
    "opt_state = optimizer.init(rho_0)\n",
    "\n",
    "def step(rho_n, opt_state, x, t, x_in, x_out, sigma_1, v_max, rho_max, a, b):\n",
    "    loss, grads = jax.value_and_grad(total_loss)(rho_n, x, t, x_in, x_out, sigma_1, v_max, rho_max, a, b)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state, rho_n)\n",
    "    rho_n = eqx.apply_updates(rho_n, updates)\n",
    "    return rho_n, opt_state, loss\n",
    "\n",
    "x = jnp.linspace(0, L, N).reshape(-1, 1)\n",
    "t = jnp.linspace(0, T, time_steps).reshape(-1, 1)\n",
    "x_in = jnp.array([[0]])\n",
    "x_out = jnp.array([[L]])\n",
    "\n",
    "for epoch in range(1000):\n",
    "    rho_0, opt_state, loss_value = step(rho_0, opt_state, x, t, x_in, x_out, sigma_1, v_max, rho_max, a, b)\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss_value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 0.5**2\n",
    "\n",
    "\n",
    "L = 3\n",
    "T = 20\n",
    "\n",
    "a = 0.9\n",
    "b = 0.975\n",
    "\n",
    "\n",
    "v_max = 2\n",
    "p_max = 1 # rho_max cannot be learned Susana's paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE Loss: 6.453939249695395e-08\n",
      "Boundary Condition Loss (Left): 1.0498206615447998\n",
      "Boundary Condition Loss (Right): 0.17961753904819489\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import equinox as eqx\n",
    "\n",
    "# MLP class definition provided by the user\n",
    "class MLP(eqx.Module):\n",
    "    layers: list\n",
    "\n",
    "    def __init__(self, key, input_dim, hidden_dims, n_layers, output_dim):\n",
    "        keys = jax.random.split(key, n_layers + 1)\n",
    "        dims = [input_dim] + [hidden_dims] * n_layers + [output_dim]\n",
    "        self.layers = [eqx.nn.Linear(dims[i], dims[i + 1], key=keys[i]) for i in range(len(dims) - 1)]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = jax.nn.sigmoid(layer(x))\n",
    "        x = self.layers[-1](x)\n",
    "        return x\n",
    "\n",
    "# Define the PDE loss function\n",
    "def pde_loss(model, input, sigma, v_max, rho_max):\n",
    "    rho, grad = jax.vmap(jax.value_and_grad(model))(input)\n",
    "    hess = jax.vmap(jax.hessian(model))(input)\n",
    "\n",
    "    dp_dx = grad[:, 0]\n",
    "    dp_dt = grad[:, 1]\n",
    "    d2p_dx2 = hess[:, 0, 0]\n",
    "\n",
    "    f_rho = v_max * (1 - rho / rho_max)\n",
    "    df_rho = -v_max / rho_max\n",
    "\n",
    "    return jnp.mean((dp_dt - sigma * d2p_dx2 + dp_dx * (f_rho + rho * df_rho))**2)\n",
    "\n",
    "# Define the boundary condition loss function at x = 0\n",
    "def bc_loss_left(model, t_vals, a, sigma, v_max, rho_max):\n",
    "    input = jnp.stack([jnp.zeros_like(t_vals), t_vals], axis=1)\n",
    "    rho, grad = jax.vmap(jax.value_and_grad(model))(input)\n",
    "\n",
    "    dp_dx = grad[:, 0]\n",
    "    f_rho = v_max * (1 - rho / rho_max)\n",
    "\n",
    "    return jnp.mean((dp_dx * sigma - f_rho * rho - a * (rho_max - rho))**2)\n",
    "\n",
    "# Define the boundary condition loss function at x = L\n",
    "def bc_loss_right(model, t_vals, L, b, sigma, v_max, rho_max):\n",
    "    input = jnp.stack([jnp.ones_like(t_vals) * L, t_vals], axis=1)\n",
    "    rho, grad = jax.vmap(jax.value_and_grad(model))(input)\n",
    "\n",
    "    dp_dx = grad[:, 0]\n",
    "    f_rho = v_max * (1 - rho / rho_max)\n",
    "\n",
    "    return jnp.mean((dp_dx * sigma - f_rho * rho - b * rho)**2)\n",
    "\n",
    "# Example usage\n",
    "key = jax.random.PRNGKey(0)\n",
    "model = MLP(key, input_dim=2, hidden_dims=64, n_layers=3, output_dim='scalar')\n",
    "\n",
    "# Define the inputs\n",
    "x = jnp.linspace(0, L, 100)\n",
    "t = jnp.linspace(0, T, 100)\n",
    "xv, tv = jnp.meshgrid(x, t)\n",
    "input = jnp.stack([xv.flatten(), tv.flatten()], axis=1)\n",
    "\n",
    "# Calculate losses\n",
    "pde_loss_val = pde_loss(model, input, sigma, v_max, rho_max)\n",
    "bc_loss_left_val = bc_loss_left(model, t, a, sigma, v_max, rho_max)\n",
    "bc_loss_right_val = bc_loss_right(model, t, L, b, sigma, v_max, rho_max)\n",
    "\n",
    "print(f\"PDE Loss: {pde_loss_val}\")\n",
    "print(f\"Boundary Condition Loss (Left): {bc_loss_left_val}\")\n",
    "print(f\"Boundary Condition Loss (Right): {bc_loss_right_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.7285966873168945\n",
      "Epoch 100, Loss: 0.15413591265678406\n",
      "Epoch 200, Loss: 0.15089228749275208\n",
      "Epoch 300, Loss: 0.14911772310733795\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 42\u001b[0m\n\u001b[0;32m     39\u001b[0m t_vals \u001b[38;5;241m=\u001b[39m tv\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m trained_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrho_max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Print final loss\u001b[39;00m\n\u001b[0;32m     45\u001b[0m final_loss \u001b[38;5;241m=\u001b[39m total_loss(trained_model, x_vals, t_vals, sigma, v_max, rho_max, a, b, L)\n",
      "Cell \u001b[1;32mIn[22], line 24\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, x_vals, t_vals, sigma, v_max, rho_max, a, b, L, epochs, learning_rate)\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model, opt_state, loss\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m---> 24\u001b[0m     model, opt_state, loss \u001b[38;5;241m=\u001b[39m \u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_vals\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_vals\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m epoch \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m100\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     26\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\andre\\OneDrive\\Documentos\\Warwick\\Individual Project\\Code\\Warwick-Final-Project\\venv\\Lib\\site-packages\\equinox\\_module.py:910\u001b[0m, in \u001b[0;36m_unflatten_module\u001b[1;34m(cls, aux, dynamic_field_values)\u001b[0m\n\u001b[0;32m    900\u001b[0m     aux \u001b[38;5;241m=\u001b[39m _FlattenedData(\n\u001b[0;32m    901\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(dynamic_field_names),\n\u001b[0;32m    902\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(static_field_names),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    905\u001b[0m         \u001b[38;5;28mtuple\u001b[39m(wrapper_field_values),\n\u001b[0;32m    906\u001b[0m     )\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(dynamic_field_values), aux\n\u001b[1;32m--> 910\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_unflatten_module\u001b[39m(\u001b[38;5;28mcls\u001b[39m: \u001b[38;5;28mtype\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m\"\u001b[39m], aux: _FlattenedData, dynamic_field_values):\n\u001b[0;32m    911\u001b[0m     \u001b[38;5;66;03m# This doesn't go via `__init__`. A user may have done something nontrivial there,\u001b[39;00m\n\u001b[0;32m    912\u001b[0m     \u001b[38;5;66;03m# and the field values may be dummy values as used in various places throughout JAX.\u001b[39;00m\n\u001b[0;32m    913\u001b[0m     \u001b[38;5;66;03m# See also\u001b[39;00m\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;66;03m# https://jax.readthedocs.io/en/latest/pytrees.html#custom-pytrees-and-initialization,\u001b[39;00m\n\u001b[0;32m    915\u001b[0m     \u001b[38;5;66;03m# which was (I believe) inspired by Equinox's approach here.\u001b[39;00m\n\u001b[0;32m    916\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m    917\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, value \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(aux\u001b[38;5;241m.\u001b[39mdynamic_field_names, dynamic_field_values):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optax\n",
    "\n",
    "# Total loss function\n",
    "def total_loss(model, x_vals, t_vals, sigma, v_max, rho_max, a, b, L):\n",
    "    input = jnp.stack([x_vals, t_vals], axis=1)\n",
    "    pde_loss_val = pde_loss(model, input, sigma, v_max, rho_max)\n",
    "    bc_loss_left_val = bc_loss_left(model, t_vals, a, sigma, v_max, rho_max)\n",
    "    bc_loss_right_val = bc_loss_right(model, t_vals, L, b, sigma, v_max, rho_max)\n",
    "    return pde_loss_val + bc_loss_left_val + bc_loss_right_val\n",
    "\n",
    "# Training procedure\n",
    "def train(model, x_vals, t_vals, sigma, v_max, rho_max, a, b, L, epochs, learning_rate):\n",
    "    optimizer = optax.adam(learning_rate)\n",
    "    opt_state = optimizer.init(model)\n",
    "\n",
    "    @jax.jit\n",
    "    def step(model, opt_state, x_vals, t_vals):\n",
    "        loss, grads = jax.value_and_grad(total_loss)(model, x_vals, t_vals, sigma, v_max, rho_max, a, b, L)\n",
    "        updates, opt_state = optimizer.update(grads, opt_state)\n",
    "        model = eqx.apply_updates(model, updates)\n",
    "        return model, opt_state, loss\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model, opt_state, loss = step(model, opt_state, x_vals, t_vals)\n",
    "        if epoch % 100 == 0:\n",
    "            print(f\"Epoch {epoch}, Loss: {loss}\")\n",
    "\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "key = jax.random.PRNGKey(0)\n",
    "model = MLP(key, input_dim=2, hidden_dims=64, n_layers=3, output_dim='scalar')\n",
    "\n",
    "# Define the inputs\n",
    "x = jnp.linspace(0, L, 100)\n",
    "t = jnp.linspace(0, T, 100)\n",
    "xv, tv = jnp.meshgrid(x, t)\n",
    "x_vals = xv.flatten()\n",
    "t_vals = tv.flatten()\n",
    "\n",
    "# Train the model\n",
    "trained_model = train(model, x_vals, t_vals, sigma, v_max, rho_max, a, b, L, epochs=1000, learning_rate=1e-3)\n",
    "\n",
    "# Print final loss\n",
    "final_loss = total_loss(trained_model, x_vals, t_vals, sigma, v_max, rho_max, a, b, L)\n",
    "print(f\"Final Loss: {final_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDE Loss: 0.0009129050886258483\n",
      "Boundary Condition Loss (Left): 0.5765161514282227\n",
      "Boundary Condition Loss (Right): 0.15116766095161438\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Calculate losses\n",
    "pde_loss_val = pde_loss(model, input, sigma, v_max, rho_max)\n",
    "bc_loss_left_val = bc_loss_left(model, t, a, sigma, v_max, rho_max)\n",
    "bc_loss_right_val = bc_loss_right(model, t, L, b, sigma, v_max, rho_max)\n",
    "\n",
    "print(f\"PDE Loss: {pde_loss_val}\")\n",
    "print(f\"Boundary Condition Loss (Left): {bc_loss_left_val}\")\n",
    "print(f\"Boundary Condition Loss (Right): {bc_loss_right_val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
